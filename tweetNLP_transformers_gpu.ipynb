{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6408ff59-fe79-4335-80ae-9700135e7863",
   "metadata": {},
   "source": [
    "# Can we predict disasters from tweets using NLP?\n",
    "This is a Kaggle's \"Getting Started\" competition. I used [a RNN model from TensorFlow](https://www.tensorflow.org/text/tutorials/text_classification_rnn) as the NLP model to solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1251ae1-9815-40a8-af44-dd98fbf0afd1",
   "metadata": {},
   "source": [
    "## Section 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2cc7df3-07c1-4bce-8a0e-632082b336fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model,layers,optimizers,callbacks,losses,metrics,utils\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "is_clean = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4f4f929-6c97-4536-ba37-c14dd2437c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max number of words is 31\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "train_df = train_df.drop(columns=['id','keyword','location'],axis=1)\n",
    "test_df = test_df.drop(columns=['keyword','location'],axis=1)\n",
    "print('max number of words is',max([len(x.split()) for x in train_df['text']]))\n",
    "max_length=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c96f656-4bd6-442a-9bc3-03ff2c40cdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Our Deeds are the Reason of this #earthquake M...\n",
      "1               Forest fire near La Ronge Sask. Canada\n",
      "2    All residents asked to 'shelter in place' are ...\n",
      "3    13,000 people receive #wildfires evacuation or...\n",
      "4    Just got sent this photo from Ruby #Alaska as ...\n",
      "5    #RockyFire Update => California Hwy. 20 closed...\n",
      "6    #flood #disaster Heavy rain causes flash flood...\n",
      "7    I'm on top of the hill and I can see a fire in...\n",
      "8    There's an emergency evacuation happening now ...\n",
      "9    I'm afraid that the tornado is coming to our a...\n",
      "Name: text, dtype: object\n",
      "0    Our Deeds are the Reason of this #earthquake M...\n",
      "1               Forest fire near La Ronge Sask. Canada\n",
      "2    All residents asked to 'shelter in place' are ...\n",
      "3    13,000 people receive #wildfires evacuation or...\n",
      "4    Just got sent this photo from Ruby #Alaska as ...\n",
      "5    #RockyFire Update => California Hwy. 20 closed...\n",
      "6    #flood #disaster Heavy rain causes flash flood...\n",
      "7    I'm on top of the hill and I can see a fire in...\n",
      "8    There's an emergency evacuation happening now ...\n",
      "9    I'm afraid that the tornado is coming to our a...\n",
      "Name: text, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19151/3375056017.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df.text[j] = process_tweet(tweet)\n",
      "/tmp/ipykernel_19151/3375056017.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df.text[j] = process_tweet(tweet)\n"
     ]
    }
   ],
   "source": [
    "# data cleaning\n",
    "import string, re, os\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    # remove stock market tickers like $GE\n",
    "    #tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    #tweet = re.sub(r'#', '', tweet)\n",
    "    #tweet = re.sub('\\n', '', tweet)\n",
    "    # remove numbers\n",
    "    # tweet = re.sub('\\w*\\d\\w*', '', tweet)\n",
    "    return tweet\n",
    "\n",
    "if is_clean:\n",
    "    print(train_df.text[:10])\n",
    "    for j,tweet in enumerate(train_df.text):\n",
    "        train_df.text[j] = process_tweet(tweet)\n",
    "    print(train_df.text[:10])\n",
    "    for j,tweet in enumerate(test_df.text):\n",
    "         test_df.text[j] = process_tweet(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c57a003-c6d7-4fa2-b1a4-feccb8ad23c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "108/108 [==============================] - 52s 292ms/step - loss: 0.4981 - accuracy: 0.7628 - val_loss: 0.3921 - val_accuracy: 0.8294\n",
      "Epoch 2/2\n",
      "108/108 [==============================] - 28s 256ms/step - loss: 0.3579 - accuracy: 0.8625 - val_loss: 0.3910 - val_accuracy: 0.8228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2aae7799f790>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model setup & training\n",
    "x_train=tokenizer(\n",
    "    text=train_df['text'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_length,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='tf',\n",
    "    return_attention_mask=True,\n",
    "    verbose=True   \n",
    ")\n",
    "input_ids=layers.Input(shape=(max_length,),dtype=tf.int32,name='input_ids')\n",
    "attention_mask=layers.Input(shape=(max_length,),dtype=tf.int32,name='attention_mask')\n",
    "embedding=bert(input_ids,attention_mask)[1]\n",
    "\n",
    "# out=layers.Dropout(0.1)(embedding)\n",
    "# out=layers.Dense(64,activation='relu')(out)\n",
    "out=layers.Dense(64,activation='relu')(embedding)\n",
    "\n",
    "# out=layers.Dropout(0.1)(out)\n",
    "#out=layers.Dense(64,activation='relu')(out)\n",
    "out=layers.Dense(16,activation='relu')(out)\n",
    "y=layers.Dense(1,activation='sigmoid')(out)\n",
    "\n",
    "nn = Model(inputs=[input_ids,attention_mask],outputs=y)\n",
    "nn.layers[2].trainable = True\n",
    "optimizer = optimizers.Adam(learning_rate = 1e-4)\n",
    "\n",
    "y_train=train_df['target']\n",
    "\n",
    "nn.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "nn.fit(\n",
    "    x={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']},\n",
    "    y=y_train,\n",
    "    validation_split=0.01,\n",
    "    epochs=2,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3158843-17d8-4f1d-ab4c-9b97997489ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "x_test=tokenizer(\n",
    "    text=test_df.text.tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_length,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='tf',\n",
    "    return_attention_mask=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "predicted = nn.predict({'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']})\n",
    "predictions = np.where(predicted>0.5,1,0)\n",
    "test_id = test_df['id'].tolist()\n",
    "predictions = predictions.reshape(-1,)\n",
    "df = pd.DataFrame(data={'id':test_id,'target': np.transpose(predictions)})\n",
    "df.head()\n",
    "df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e1dda6a-a862-41cc-addc-2d393b4cdb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model with the training data\n",
    "# predicted = nn.predict({'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']})\n",
    "# predictions = np.where(predicted>0.5,1,0)\n",
    "# predictions = predictions.reshape(-1,)\n",
    "# acc = sum(predictions==y_train)/len(y_train)*100\n",
    "# acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dd4e96-972c-4d2a-91e0-39d24b9b7ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
